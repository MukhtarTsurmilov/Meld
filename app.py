import os
import re
import html
import joblib
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.metrics.pairwise import linear_kernel
from rapidfuzz import process, fuzz
import pymorphy3
from scipy import sparse
import gdown

# üî• –ù–ê–°–¢–†–û–ô–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ê–†–¢–ï–§–ê–ö–¢–û–í –ò–ó GOOGLE DRIVE
ARTIFACT_DIR = "artifacts"
os.makedirs(ARTIFACT_DIR, exist_ok=True)

# üî• –ó–ê–ú–ï–ù–ò –°–°–´–õ–ö–ò –ù–ê –¢–í–û–ò –§–ê–ô–õ–´ –ò–ó GOOGLE DRIVE
files_to_download = [
    ("data_arrays.npz", "https://drive.google.com/uc?id=1J2aZ4Din2s3W2JVlROH7A3PhPkhzZTOA"),
    ("vectorizer.joblib", "https://drive.google.com/uc?id=1t9UjgGZfCYubSZbyip_UNzEVSRgEzqm8"),
    ("tfidf_matrix.joblib", "https://drive.google.com/uc?id=1trO2RiHvggQkNzmLWDWdCaku9DvXnjvv"),
    ("vectorizer_lemma.joblib", "https://drive.google.com/uc?id=1p2iYFFxxYnCZKVL7irBr2KP-PggoG8Fa"),
    ("tfidf_matrix_lemma.joblib", "https://drive.google.com/uc?id=1ZynEXjWfp-cl00f2oIOjuE98UOnMynU7"),
]


st.info("üöÄ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ, –Ω–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤...")

for filename, url in files_to_download:
    st.info(f"üì• –ü—ã—Ç–∞—é—Å—å —Å–∫–∞—á–∞—Ç—å {filename}...")
    filepath = os.path.join(ARTIFACT_DIR, filename)
    if not os.path.exists(filepath):
        st.info(f"üì• –°–∫–∞—á–∏–≤–∞—é {filename}...")
        gdown.download(url, filepath, quiet=False)
        st.success(f"‚úÖ {filename} —Å–∫–∞—á–∞–Ω")
    else:
        st.success(f"‚úÖ {filename} —É–∂–µ –µ—Å—Ç—å")

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def normalize(text):
    if not isinstance(text, str):
        return ""
    text = text.lower().replace("—ë", "–µ")
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø—è—Ç—ã–µ –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏
    while re.search(r'\d,\d', text):
        text = re.sub(r'(\d),(\d)', r'\1_TEMP_COMMA_\2', text)
    # –£–¥–∞–ª—è–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r"[^\w\s\-\.\/+]", " ", text)
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—è—Ç—ã–µ
    text = text.replace('_TEMP_COMMA_', ',')
    return re.sub(r"\s+", " ", text).strip()

@st.cache_resource
def get_morph():
    return pymorphy3.MorphAnalyzer()

def lemmatize_text(text, morph):
    """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not isinstance(text, str):
        return ""
    word_lemma_cache = {}
    def lemmatize_word(word):
        if len(word) <= 2:
            return word
        if word in word_lemma_cache:
            return word_lemma_cache[word]
        try:
            lemma = morph.parse(word)[0].normal_form
        except:
            lemma = word
        word_lemma_cache[word] = lemma
        return lemma
    return " ".join(lemmatize_word(w) for w in text.split())

def get_all_word_forms(word: str):
    morph = get_morph()
    forms = set()
    try:
        parses = morph.parse(word)
        for p in parses:
            for wf in p.lexeme:
                forms.add(wf.word.lower())
    except:
        pass
    forms.add(word.lower())
    return forms

def highlight_morphological_matches(text: str, query: str) -> str:
    if not query.strip():
        return html.escape(text)
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    q_norm = normalize(query)
    query_words = q_norm.split()
    if not query_words:
        return html.escape(text)
    highlighted = html.escape(text)
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ ‚Äî –∏—â–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
    for word in query_words:
        if len(word) < 2:
            continue
        escaped_word = re.escape(word)
        matches = list(re.finditer(escaped_word, highlighted, re.IGNORECASE))
        for match in reversed(matches):
            start, end = match.span()
            original_fragment = highlighted[start:end]
            highlighted = highlighted[:start] + f"<strong>{original_fragment}</strong>" + highlighted[end:]
    return highlighted

def artifacts_exist(dir_path):
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è NPZ –∞—Ä—Ö–∏–≤–∞
    required_files = ["vectorizer.joblib", "tfidf_matrix.joblib", "data_arrays.npz"]
    return all(os.path.exists(os.path.join(dir_path, f)) for f in required_files)

@st.cache_resource
def load_index(artifacts_dir=ARTIFACT_DIR):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º joblib —Ñ–∞–π–ª—ã
    vectorizer = joblib.load(os.path.join(artifacts_dir, "vectorizer.joblib"))
    X = joblib.load(os.path.join(artifacts_dir, "tfidf_matrix.joblib"))
    # üî• –ó–ê–ì–†–£–ó–ö–ê –ò–ó NPZ –ê–†–•–ò–í–ê –≤–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö .npy
    npz_path = os.path.join(artifacts_dir, "data_arrays.npz")
    if os.path.exists(npz_path):
        with np.load(npz_path, allow_pickle=True) as data:
            names = data['names']
            texts = data['texts']
            ids = data.get('ids', None)
            texts_lemmatized = data.get('texts_lemmatized', None)
    else:
        # Fallback –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        st.warning("NPZ –∞—Ä—Ö–∏–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—ã—Ç–∞—é—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ NPY —Ñ–∞–π–ª—ã...")
        names = np.load(os.path.join(artifacts_dir, "names.npy"), allow_pickle=True)
        texts = np.load(os.path.join(artifacts_dir, "texts.npy"), allow_pickle=True)
        ids_path = os.path.join(artifacts_dir, "ids.npy")
        ids = np.load(ids_path, allow_pickle=True) if os.path.exists(ids_path) else None
        lemmatized_path = os.path.join(artifacts_dir, "texts_lemmatized.npy")
        texts_lemmatized = np.load(lemmatized_path, allow_pickle=True) if os.path.exists(lemmatized_path) else None
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã ‚Äî –µ—Å–ª–∏ –µ—Å—Ç—å
    vectorizer_lemma = None
    X_lemma = None
    vectorizer_lemma_path = os.path.join(artifacts_dir, "vectorizer_lemma.joblib")
    if os.path.exists(vectorizer_lemma_path):
        vectorizer_lemma = joblib.load(vectorizer_lemma_path)
    tfidf_lemma_path = os.path.join(artifacts_dir, "tfidf_matrix_lemma.joblib")
    if os.path.exists(tfidf_lemma_path):
        X_lemma = joblib.load(tfidf_lemma_path)

    # üî• –ü–†–û–í–ï–†–ö–ê: –ï—Å–ª–∏ X_lemma –µ—Å—Ç—å, –Ω–æ vectorizer_lemma –Ω–µ—Ç ‚Äî –æ–±–Ω—É–ª—è–µ–º X_lemma
    if X_lemma is not None and vectorizer_lemma is None:
        st.warning("‚ö†Ô∏è –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –ª–µ–º–º, –Ω–æ –Ω–µ—Ç –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞ ‚Äî –∏–≥–Ω–æ—Ä–∏—Ä—É—é –ª–µ–º–º—ã")
        X_lemma = None

    return vectorizer, X, names, texts, ids, vectorizer_lemma, X_lemma, texts_lemmatized

def get_candidates(query, top_k, vectorizer, X, names, texts, 
                   vectorizer_lemma=None, X_lemma=None, texts_lemmatized=None):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å —Ä–∞–∑–¥–µ–ª—å–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π"""
    q_norm = normalize(query)
    morph = get_morph()
    #–†–ê–ó–î–ï–õ–¨–ù–´–ô –ü–û–ò–°–ö –° –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ï–ô
    if vectorizer_lemma is not None and X_lemma is not None and texts_lemmatized is not None:
        # 1. –ü–æ–∏—Å–∫ –ø–æ –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º (50% –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
        q_vec_standard = vectorizer.transform([q_norm])
        sims_standard = linear_kernel(X, q_vec_standard).ravel()
        # 2. –ü–æ–∏—Å–∫ –ø–æ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º (50% –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
        q_lemma = lemmatize_text(q_norm, morph)
        q_vec_lemma = vectorizer_lemma.transform([q_lemma])
        sims_lemma = linear_kernel(X_lemma, q_vec_lemma).ravel()
        # 3. –ë–µ—Ä–µ–º —Ä–∞–∑–¥–µ–ª—å–Ω–æ —Ç–æ–ø-K/2 –æ—Ç –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
        k_half = max(top_k // 2, 1)  # –ú–∏–Ω–∏–º—É–º 1 –æ—Ç –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
        k_rest = top_k - k_half
        # –¢–æ–ø –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if len(sims_standard) >= k_half:
            idxs_standard = np.argpartition(-sims_standard, k_half)[:k_half]
        else:
            idxs_standard = np.argsort(-sims_standard)[:len(sims_standard)]
        # –¢–æ–ø –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –ø–æ–∏—Å–∫–∞ –ø–æ –ª–µ–º–º–∞–º
        if len(sims_lemma) >= k_rest:
            idxs_lemma = np.argpartition(-sims_lemma, k_rest)[:k_rest]
        else:
            idxs_lemma = np.argsort(-sims_lemma)[:len(sims_lemma)]
        # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
        all_indices = set(idxs_standard.tolist() + idxs_lemma.tolist())
        # 5. –ï—Å–ª–∏ –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –º–µ–Ω—å—à–µ —á–µ–º –Ω—É–∂–Ω–æ - –¥–æ–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–º–∏ –∏–∑ –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤
        if len(all_indices) < top_k:
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–∫–æ—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
            combined_scores = []
            for idx in range(len(sims_standard)):
                if idx < len(sims_lemma):  # –ó–∞—â–∏—Ç–∞ –æ—Ç –≤—ã—Ö–æ–¥–∞ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã
                    combined_score = 0.5 * sims_standard[idx] + 0.5 * sims_lemma[idx]
                    combined_scores.append((idx, combined_score))
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É –∏ –¥–æ–±–∏—Ä–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ
            combined_scores.sort(key=lambda x: x[1], reverse=True)
            for idx, score in combined_scores:
                if len(all_indices) >= top_k:
                    break
                if idx not in all_indices:
                    all_indices.add(idx)
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –º–∞—Å—Å–∏–≤
        idxs = np.array(list(all_indices))
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        final_scores = []
        for idx in idxs:
            if idx < len(sims_lemma):
                score = 0.5 * sims_standard[idx] + 0.5 * sims_lemma[idx]
            else:
                score = sims_standard[idx]  # Fallback
            final_scores.append(score)
        final_scores = np.array(final_scores)
        sorted_order = np.argsort(-final_scores)
        idxs = idxs[sorted_order]
        sims = final_scores[sorted_order]
        st.caption(f"üîç –†–∞–∑–¥–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: {len(idxs_standard)} –æ–±—ã—á–Ω—ã—Ö + {len(idxs_lemma)} –ª–µ–º–º = {len(idxs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
    else:
        # Fallback: —Ç–æ–ª—å–∫–æ –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞)
        q_vec = vectorizer.transform([q_norm])
        sims = linear_kernel(X, q_vec).ravel()
        n = len(sims)
        if top_k >= n:
            idxs = np.argsort(-sims)
        else:
            idx_part = np.argpartition(-sims, top_k)[:top_k]
            idxs = idx_part[np.argsort(-sims[idx_part])]
        st.caption("üîç –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ (–ª–µ–º–º—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)")
    return names[idxs], texts[idxs], sims, idxs

def smart_rerank(query, names, texts, sims, idxs, top_n=20, ids_full=None,
                 vectorizer_lemma=None, X_lemma=None, texts_lemmatized=None):
    if not query.strip():
        return pd.DataFrame()
    q_norm = normalize(query)
    query_words = q_norm.split()
    query_len = len(q_norm)
    # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å ‚Äî –µ—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
    q_lemma = q_norm
    if vectorizer_lemma is not None and texts_lemmatized is not None:
        try:
            morph = get_morph()
            q_lemma = lemmatize_text(q_norm, morph)
        except:
            pass
    results = []
    for i, text in enumerate(texts):
        # 1. Fuzzy score (–ø–æ —Ñ–æ—Ä–º–µ) ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π
        fuzzy_score = fuzz.partial_ratio(q_norm, text) / 100.0
        # 2. TF-IDF –ø–æ –ª–µ–º–º–∞–º (–ø–æ —Å–º—ã—Å–ª—É) ‚Äî –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π
        lemma_score = 0.0
        if vectorizer_lemma is not None and X_lemma is not None and texts_lemmatized is not None:
            try:
                q_vec = vectorizer_lemma.transform([q_lemma])
                global_idx = idxs[i] if idxs is not None else i
                if global_idx < X_lemma.shape[0]:
                    doc_vec = X_lemma[global_idx]
                    sim = linear_kernel(doc_vec, q_vec).ravel()[0]
                    lemma_score = float(sim)
            except:
                lemma_score = 0.0
        # 3. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º: 70% —Ñ–æ—Ä–º–∞ + 30% —Å–º—ã—Å–ª
        if vectorizer_lemma is not None and X_lemma is not None:
            combined_score = 0.7 * fuzzy_score + 0.3 * lemma_score
        else:
            combined_score = fuzzy_score
        # 4. –ë–æ–Ω—É—Å—ã
        bonus = 0.0
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞—á–∞–ª–æ —Å—Ç—Ä–æ–∫–∏
        if text.startswith(q_norm):
            bonus += 0.20
        # –ë–æ–Ω—É—Å –∑–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
        if f" {q_norm} " in f" {text} " or text.startswith(f"{q_norm} ") or text.endswith(f" {q_norm}"):
            bonus += 0.10
        # –ë–æ–Ω—É—Å –∑–∞ —á–∏—Å–ª–∞
        number_bonus = 0.0
        try:
            query_numbers = re.findall(r'\d+', q_norm)
            if query_numbers:
                text_numbers = re.findall(r'\d+', text)
                for q_num in query_numbers:
                    if q_num in text_numbers:
                        number_bonus += 0.15
                        break
        except:
            pass
        # –ë–û–ù–£–°: –µ—Å–ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî +20%
        all_words_present = True
        for word in query_words:
            if word not in text:
                all_words_present = False
                break
        if all_words_present and len(query_words) > 0:
            bonus += 0.20
        # 5. –®—Ç—Ä–∞—Ñ—ã
        if query_len <= 2:
            combined_score *= 0.5
        elif query_len <= 4:
            combined_score *= 0.8
        # 6. –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä
        final_score = min(combined_score * (1.0 + bonus + number_bonus), 1.0)
        results.append((i, final_score))
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º
    results.sort(key=lambda x: x[1], reverse=True)
    top_local_indices = [i for i, _ in results[:top_n]]
    global_indices = idxs[top_local_indices] if idxs is not None else np.array(top_local_indices)
    # –§–æ—Ä–º–∏—Ä—É–µ–º DataFrame
    if ids_full is not None:
        id_values = np.asarray(ids_full)[global_indices]
        id_col = "–ï–ù–°: –ö–æ–¥ –∑–∞–ø–∏—Å–∏ (ID)"
    else:
        id_values = global_indices
        id_col = "–ò–Ω–¥–µ–∫—Å"
    return pd.DataFrame({
        "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": np.asarray(names)[top_local_indices],
        id_col: id_values
    })

def render_fixed_table(df, query, name_col="–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", idx_col="–ò–Ω–¥–µ–∫—Å", idx_width_px=200):
    def esc(x):
        return html.escape(str(x)) if x is not None else ""
    css = f"""
    <style>
      .tbl-wrap {{ width: 100%; }}
      table.table-fixed {{
        width: 100%;
        border-collapse: collapse;
        table-layout: fixed;
        background: #ffffff;
      }}
      table.table-fixed col.col-name {{ width: calc(100% - {idx_width_px}px); }}
      table.table-fixed col.col-idx  {{ width: {idx_width_px}px; }}
      table.table-fixed th, td {{
        border: 1px solid #e6e6e6;
        padding: 8px 10px;
        font-size: 14px;
        line-height: 1.3;
        vertical-align: top;
      }}
      thead th {{
        background: #ffd100;
        color: #000;
        font-weight: 600;
      }}
      td.name-cell {{
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
      }}
      td.idx-cell {{
        text-align: right;
        white-space: nowrap;
      }}
      mark {{
        background: #ffeb3b !important;
        padding: 0 3px !important;
        border-radius: 3px;
        font-weight: 500;
      }}
    </style>
    """
    parts = [css, '<div class="tbl-wrap"><table class="table-fixed">']
    parts.append('<colgroup><col class="col-name"><col class="col-idx"></colgroup>')
    parts.append(f'<thead><tr><th>{esc(name_col)}</th><th>{esc(idx_col)}</th></tr></thead><tbody>')
    for _, row in df.iterrows():
        highlighted_name = highlight_morphological_matches(row[name_col], query)
        parts.append(f'<tr><td class="name-cell">{highlighted_name}</td><td class="idx-cell">{esc(row[idx_col])}</td></tr>')
    parts.append('</tbody></table></div>')
    st.markdown("".join(parts), unsafe_allow_html=True)

# –°—Ç–∏–ª–∏ –∏ UI
st.set_page_config(page_title="–ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ç–∞–ª–æ–≥—É", layout="wide")
st.markdown("""
<style>
:root{ --ros-yellow: #ffd100; --ros-gray: #f8f9fa; --ros-black: #000000; }
html, body, [data-testid="stAppViewContainer"]{ background: var(--ros-gray); color: var(--ros-black); }
[data-testid="stHeader"]{ background: var(--ros-yellow); height: 5rem; }
[data-testid="stSidebar"] { background: #4a4a4a !important; color: #e9e9e9 !important; }
[data-testid="stSidebar"] * { color: #e9e9e9 !important; }
.stButton > button{ background: var(--ros-yellow); color: var(--ros-black); border: 1px solid; border-radius: 6px; }
.stButton > button:hover{ filter: brightness(0.95); }
.stSlider div[data-baseweb="slider"] > div > div > div[style*="left: 0px"] { background: var(--ros-yellow) !important; }
div[data-testid="stAppViewContainer"] .stTextInput input{ background: #fff; color: #000; border: 1px solid #ccc; border-radius: 6px; }
div[data-testid="stAppViewContainer"] .stTextInput input:focus{ outline: none; border: 1px solid #FFD100; box-shadow: 0 0 0 2px rgba(255,209,0,0.25); }
</style>
""", unsafe_allow_html=True)

st.title("–ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ç–∞–ª–æ–≥—É")

with st.sidebar:
    st.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã")
    top_n = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π", min_value=1, max_value=200, value=20, step=1)

query = st.text_input("–ó–∞–ø—Ä–æ—Å", value="", placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä: –±–æ–ª—Ç")
btn = st.button("–ò—Å–∫–∞—Ç—å", type="primary")
top_k = 2000

try:
    vectorizer, X, names_full, texts_full, ids_full, vectorizer_lemma, X_lemma, texts_lemmatized = load_index()
    lemma_status = "–¥–æ—Å—Ç—É–ø–Ω—ã" if vectorizer_lemma is not None else "–Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã"
    st.caption(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {len(names_full):,} —Å—Ç—Ä–æ–∫. –õ–µ–º–º—ã: {lemma_status}")
except Exception as e:
    st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    st.stop()

if btn:
    if not query.strip():
        st.warning("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å.")
    else:
        with st.spinner("üîç –ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (—Ä–∞–∑–¥–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π)..."):
            names, texts, sims, idxs = get_candidates(
                query, top_k, vectorizer, X, names_full, texts_full,
                vectorizer_lemma, X_lemma, texts_lemmatized
            )
        with st.spinner("üéØ –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (70% fuzzy + 30% –ª–µ–º–º—ã)..."):
            df = smart_rerank(
                query, names, texts, sims, idxs,
                top_n=top_n,
                ids_full=ids_full,
                vectorizer_lemma=vectorizer_lemma,
                X_lemma=X_lemma,
                texts_lemmatized=texts_lemmatized
            )
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        if df.empty:
            st.info("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        else:
            id_col = "–ï–ù–°: –ö–æ–¥ –∑–∞–ø–∏—Å–∏ (ID)" if "–ï–ù–°: –ö–æ–¥ –∑–∞–ø–∏—Å–∏ (ID)" in df.columns else "–ò–Ω–¥–µ–∫—Å"
            render_fixed_table(df, query, name_col="–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", idx_col=id_col, idx_width_px=180)
